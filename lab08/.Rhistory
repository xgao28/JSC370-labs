# word = "blood"
tokens_bigram <- tokens_bigram |>
filter(str_detect(ngram, regex("\\sblood$ | ^blood\\s"))) |>
mutate(word = str_remove(ngram, "blood"),
word = str_remove_all(word, " ")) |>
group_by(word) %>%
summarise(word_frequency = n()) %>%
arrange(across(word_frequency, desc)) %>%
head(20)
tokens_bigram %>%
ggplot(aes(x = reorder(word, -word_frequency), y = word_frequency)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(title = "Top 20 Most Frequent Words",
x = "Word",
y = "Frequency") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
stopwords2 <- c(stopwords("english"), "also", "using", "use", "used")
sw_start <- paste0("^", paste(stopwords2, collapse=" |^"), "$")
sw_end <- paste0("", paste(stopwords2, collapse="$| "), "$")
tokens_bigram <- mt_samples |>
select(transcription) |>
unnest_tokens(ngram, transcription, token = "ngrams", n = 2) |>
filter(!(grepl(sw_start, ngram, ignore.case = TRUE)))|>
filter(!(grepl(sw_end, ngram, ignore.case = TRUE)))|>
filter(!(grepl("[[:digit:]]+", ngram)))|>
group_by(ngram) %>%
summarize(word_frequency = n()) %>%
arrange(across(word_frequency, desc))
tokens_bigram |>
head(20) %>%
ggplot(aes(x = reorder(ngram, -word_frequency), y = word_frequency)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(title = "Top 20 Most Frequent Words",
x = "Word",
y = "Frequency") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
library(stringr)
# e.g. patient, blood, preoperative...
# word = "blood"
tokens_blood <- tokens_bigram |>
filter(str_detect(ngram, regex("\\sblood$ | ^blood\\s"))) |>
mutate(word = str_remove(ngram, "blood"),
word = str_remove_all(word, " ")) |>
group_by(word) %>%
summarise(word_frequency = n()) %>%
arrange(across(word_frequency, desc)) %>%
head(20)
tokens_blood %>%
ggplot(aes(x = reorder(word, -word_frequency), y = word_frequency)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(title = "Top 20 Most Frequent Words",
x = "Word",
y = "Frequency") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
library(stringr)
# e.g. patient, blood, preoperative...
# word = "blood"
tokens_blood <- tokens_bigram |>
filter(str_detect(ngram, regex("\\sblood | blood\\s"))) |>
mutate(word = str_remove(ngram, "blood"),
word = str_remove_all(word, " ")) |>
group_by(word) %>%
summarise(word_frequency = n()) %>%
arrange(across(word_frequency, desc)) %>%
head(20)
tokens_blood %>%
ggplot(aes(x = reorder(word, -word_frequency), y = word_frequency)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(title = "Top 20 Most Frequent Words",
x = "Word",
y = "Frequency") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
library(stringr)
# e.g. patient, blood, preoperative...
# word = "blood"
tokens_blood <- tokens_bigram |>
filter(str_detect(ngram, regex("\\sblood$ | ^blood\\s"))) |>
mutate(word = str_remove(ngram, "blood"),
word = str_remove_all(word, " ")) |>
group_by(word) %>%
summarise(word_frequency = n()) %>%
arrange(across(word_frequency, desc))
tokens_blood %>%
head(20) %>%
ggplot(aes(x = reorder(word, -word_frequency), y = word_frequency)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(title = "Top 20 Most Frequent Words",
x = "Word",
y = "Frequency") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
library(stringr)
# e.g. patient, blood, preoperative...
# word = "blood"
tokens_blood <- tokens_bigram |>
filter(str_detect(ngram, regex("\\sblood$|^blood\\s"))) |>
mutate(word = str_remove(ngram, "blood"),
word = str_remove_all(word, " ")) |>
group_by(word) %>%
summarise(word_frequency = n()) %>%
arrange(across(word_frequency, desc))
tokens_blood %>%
head(20) %>%
ggplot(aes(x = reorder(word, -word_frequency), y = word_frequency)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(title = "Top 20 Most Frequent Words",
x = "Word",
y = "Frequency") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
mt_samples |>
unnest_tokens(word, transcription) |>
filter(!(word %in% stopwords("english"))) |>
filter(!(grepl("[[:digit:]]+", word))) %>%
group_by(medical_specialty) |>
count(word, sort = TRUE) %>%
top_n(1, n)
mt_samples |>
unnest_tokens(word, transcription) |>
filter(!(word %in% stopwords("english"))) |>
filter(!(grepl("[[:digit:]]+", word))) %>%
group_by(medical_specialty) |>
count(word, sort = TRUE) %>%
top_n(5, n)
transcripts_dtm <- mt_samples |>
select(transcription) |>
unnest_tokens() |>
filter(!(word %in% stopwords("english"))) |>
filter(!grepl("[[:digit]]+", word)) |>
DocumentTermMatrix()
transcripts_dtm <- mt_samples |>
select(transcription) |>
unnest_tokens(word, transcription) |>
filter(!(word %in% stopwords("english"))) |>
filter(!grepl("[[:digit]]+", word)) |>
DocumentTermMatrix()
transcripts_dtm <- mt_samples |>
select(transcription) |>
unnest_tokens(word, transcription) |>
filter(!(word %in% stopwords("english"))) |>
filter(!grepl("[[:digit:]]+", word)) |>
DocumentTermMatrix()
transcripts_dtm <- as.matrix(transcripts_dtm)
transcripts_lda <- LDA()
View(transcripts_dtm)
transcripts_dtm <- mt_samples |>
select(transcription) |>
unnest_tokens(word, transcription) |>
filter(!(word %in% stopwords("english"))) |>
filter(!grepl("[[:digit:]]+", word)) |>
DocumentTermMatrix()
transcripts_dtm <- as.matrix(transcripts_dtm)
transcripts_lda <- LDA(transcripts_dtm, k=5, control = list(seed = 1234))
transcripts_lda
transcripts_tops_terms <-
tidy(transcripts_lda, matrix='beta') %>%
group_by(topics) %>%
slice_max(beta, n=10) %>%
ungroup() %>%
arrange(topic, beta)
library(broom)
transcripts_tops_terms <-
tidy(transcripts_lda, matrix='beta') %>%
group_by(topics) %>%
slice_max(beta, n=10) %>%
ungroup() %>%
arrange(topic, beta)
View(tokens_blood)
library(stringr)
# e.g. patient, blood, preoperative...
# word = "blood"
tokens_blood <- tokens_bigram |>
filter(str_detect(ngram, regex("\\sblood$|^blood\\s"))) |>
mutate(word = str_remove(ngram, "blood"),
word = str_remove_all(word, " ")) |>
group_by(word) %>%
summarise(word_frequency = n()) %>%
arrange(across(word_frequency, desc))
tokens_blood %>%
head(20) %>%
ggplot(aes(x = reorder(word, -word_frequency), y = word_frequency)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(title = "Top 20 Most Frequent Words",
x = "Word",
y = "Frequency") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
tokens_blood <- tokens_bigram |>
filter(str_detect(ngram, regex("\\sblood$|^blood\\s"))) |>
mutate(word = str_remove(ngram, "blood"),
word = str_remove_all(word, " ")) |>
group_by(word) %>%
summarise(word_frequency = n()) %>%
arrange(across(word_frequency, desc))
tokens_blood <- tokens_bigram |>
filter(str_detect(ngram, regex("\\sblood$|^blood\\s")))
View(tokens_blood)
library(stringr)
# e.g. patient, blood, preoperative...
# word = "blood"
tokens_blood <- tokens_bigram |>
filter(str_detect(ngram, regex("\\sblood$|^blood\\s"))) |>
mutate(word = str_remove(ngram, "blood"),
word = str_remove_all(word, " ")) |>
group_by(word) %>%
summarise(word_frequency = n()) %>%
arrange(across(word_frequency, desc))
tokens_blood %>%
head(20) %>%
ggplot(aes(x = reorder(word, -word_frequency), y = word_frequency)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(title = "Top 20 Most Frequent Words",
x = "Word",
y = "Frequency") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
library(stringr)
# e.g. patient, blood, preoperative...
# word = "blood"
tokens_blood <- tokens_bigram |>
filter(str_detect(ngram, regex("\\sblood$|^blood\\s"))) |>
mutate(word = str_remove(ngram, "blood")) %>%
#word = str_remove_all(word, " ")) |>
group_by(word) %>%
summarise(word_frequency = n()) %>%
arrange(across(word_frequency, desc))
tokens_blood %>%
head(20) %>%
ggplot(aes(x = reorder(word, -word_frequency), y = word_frequency)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(title = "Top 20 Most Frequent Words",
x = "Word",
y = "Frequency") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
library(stringr)
# e.g. patient, blood, preoperative...
# word = "blood"
tokens_blood <- tokens_bigram |>
filter(str_detect(ngram, regex("\\sblood|blood\\s"))) |>
mutate(word = str_remove(ngram, "blood")
word = str_remove_all(word, " ")) |>
library(stringr)
# e.g. patient, blood, preoperative...
# word = "blood"
tokens_blood <- tokens_bigram |>
filter(str_detect(ngram, regex("\\sblood|blood\\s"))) |>
mutate(word = str_remove(ngram, "blood"),
word = str_remove_all(word, " ")) |>
group_by(word) %>%
summarise(word_frequency = n()) %>%
arrange(across(word_frequency, desc))
tokens_blood %>%
head(20) %>%
ggplot(aes(x = reorder(word, -word_frequency), y = word_frequency)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(title = "Top 20 Most Frequent Words",
x = "Word",
y = "Frequency") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
tokens_blood <- tokens_bigram |>
filter(str_detect(ngram, regex("\\sblood|blood\\s"))) |>
mutate(word = str_remove(ngram, "blood"),
word = str_remove_all(word, " "))
library(stringr)
# e.g. patient, blood, preoperative...
# word = "blood"
tokens_blood <- tokens_bigram |>
filter(str_detect(ngram, regex("\\sblood|blood\\s"))) |>
mutate(word = str_remove(ngram, "blood"),
word = str_remove_all(word, " ")) |>
arrange(across(word_frequency, desc))
tokens_blood %>%
head(20) %>%
ggplot(aes(x = reorder(word, -word_frequency), y = word_frequency)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(title = "Top 20 Most Frequent Words",
x = "Word",
y = "Frequency") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
library(stringr)
# e.g. patient, blood, preoperative...
# word = "blood"
tokens_blood <- tokens_bigram |>
filter(str_detect(ngram, regex("\\sblood$|^blood\\s"))) |>
mutate(word = str_remove(ngram, "blood"),
word = str_remove_all(word, " ")) |>
arrange(across(word_frequency, desc))
tokens_blood %>%
head(20) %>%
ggplot(aes(x = reorder(word, -word_frequency), y = word_frequency)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(title = "Top 20 Most Frequent Words",
x = "Word",
y = "Frequency") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
mt_samples |>
unnest_tokens(word, transcription) |>
filter(!(word %in% stopwords("english"))) |>
filter(!(grepl("[[:digit:]]+", word))) %>%
group_by(medical_specialty) |>
count(word, sort = TRUE) %>%
top_n(1, n)
mt_samples |>
unnest_tokens(word, transcription) |>
filter(!(word %in% stopwords("english"))) |>
filter(!(grepl("[[:digit:]]+", word))) %>%
group_by(medical_specialty) |>
count(word, sort = TRUE) %>%
top_n(5, n)
transcripts_dtm <- mt_samples |>
select(transcription) |>
unnest_tokens(word, transcription) |>
filter(!(word %in% stopwords("english"))) |>
filter(!grepl("[[:digit:]]+", word)) |>
DocumentTermMatrix()
transcripts_dtm <- as.matrix(transcripts_dtm)
transcripts_lda <- LDA(transcripts_dtm, k=5, control = list(seed = 1234))
transcripts_lda
library(broom)
transcripts_tops_terms <-
tidy(transcripts_lda, matrix='beta') %>%
group_by(topics) %>%
slice_max(beta, n=10) %>%
ungroup() %>%
arrange(topic, beta)
help(reshape2)
??reshape2
library(broom)
transcripts_tops_terms <-
tidy(transcripts_lda, matrix='beta') %>%
group_by(topics) %>%
slice(beta, n=10) %>%
ungroup() %>%
arrange(topic, beta)
library(broom)
transcripts_tops_terms <-
tidy(transcripts_lda, matrix='beta')
library(broom)
transcripts_tops_terms <- tidy(transcripts_lda, matrix='beta') %>%
group_by(topics) %>%
slice(beta, n=10) %>%
ungroup() %>%
arrange(topic, beta)
library(broom)
library(ggplot2)
library(tidyr)
library(dplyr)
transcripts_tops_terms <- tidy(transcripts_lda, matrix='beta') %>%
group_by(topics) %>%
slice(beta, n=10) %>%
ungroup() %>%
arrange(topic, beta)
# Extract top terms for each topic
top_terms <- tidy(transcripts_lda, matrix = "beta") %>%
group_by(topic) %>%
top_n(10, beta) %>%
arrange(topic, -beta)
??tidy
install.packages("broom")
install.packages("broom")
knitr::opts_chunk$set(eval = F, include  = T)
library(broom)
library(ggplot2)
library(tidyr)
library(dplyr)
transcripts_tops_terms <- tidy(transcripts_lda, matrix='beta') %>%
group_by(topics) %>%
slice(beta, n=10) %>%
ungroup() %>%
arrange(topic, beta)
# Extract top terms for each topic
top_terms <- tidy(transcripts_lda, matrix = "beta") %>%
group_by(topic) %>%
top_n(10, beta) %>%
arrange(topic, -beta)
transcripts_tops_terms <- tidy(transcripts_dtm, matrix='beta') %>%
group_by(topics) %>%
slice(beta, n=10) %>%
ungroup() %>%
arrange(topic, beta)
View(transcripts_lda)
View(transcripts_dtm)
library(broom)
library(ggplot2)
library(tidyr)
library(dplyr)
transcripts_tops_terms <- tidy(transcripts_dtm, matrix='beta') %>%
group_by(topic) %>%
slice(beta, n=10) %>%
ungroup() %>%
arrange(topic, beta)
library(broom)
library(ggplot2)
library(tidyr)
library(dplyr)
transcripts_tops_terms <- tidy(transcripts_lda, matrix='beta') %>%
group_by(topic) %>%
slice(beta, n=10) %>%
ungroup() %>%
arrange(topic, beta)
library(broom)
library(ggplot2)
library(tidyr)
library(dplyr)
# transcripts_tops_terms <- tidy(transcripts_lda, matrix='beta') %>%
# Function to extract top terms for each topic from LDA model
get_top_terms <- function(lda_model, n = 10) {
beta <- as.data.frame(terms(lda_model, n))
beta$topic <- factor(1:nrow(beta))
return(beta)
}
# Extract top terms for each topic
top_terms <- get_top_terms(transcripts_lda, n = 10)
# Plotting
ggplot(top_terms, aes(x = reorder(term, -beta), y = beta, fill = topic)) +
geom_bar(stat = "identity") +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
labs(x = NULL, y = "Beta", fill = "Topic") +
theme_minimal() +
theme(legend.position = "none") +
ggtitle("Top Terms for Each Topic")
View(top_terms)
library(broom)
library(ggplot2)
library(tidyr)
library(dplyr)
# transcripts_tops_terms <- tidy(transcripts_lda, matrix='beta') %>%
# Function to extract top terms for each topic from LDA model
get_top_terms <- function(lda_model, n = 5) {
beta <- as.data.frame(terms(lda_model, n))
beta$topic <- factor(1:nrow(beta))
return(beta)
}
# Extract top terms for each topic
top_terms <- get_top_terms(transcripts_lda, n = 5)
# Plotting
ggplot(top_terms, aes(x = reorder(term, -beta), y = beta, fill = topic)) +
geom_bar(stat = "identity") +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
labs(x = NULL, y = "Beta", fill = "Topic") +
theme_minimal() +
theme(legend.position = "none") +
ggtitle("Top Terms for Each Topic")
library(broom)
library(ggplot2)
library(tidyr)
library(dplyr)
transcripts_tops_terms <- tidy(transcripts_lda$terms, matrix='beta') %>%
group_by(topic) %>%
slice(beta, n=10) %>%
ungroup() %>%
arrange(topic, beta)
library(broom)
library(ggplot2)
library(tidyr)
library(dplyr)
transcripts_tops_terms <- tidy(transcripts_lda.terms, matrix='beta') %>%
group_by(topic) %>%
slice(beta, n=10) %>%
ungroup() %>%
arrange(topic, beta)
library(broom)
library(ggplot2)
library(tidyr)
library(dplyr)
transcripts_tops_terms <- tidy(transcripts_lda@terms, matrix='beta') %>%
group_by(topic) %>%
slice(beta, n=10) %>%
ungroup() %>%
arrange(topic, beta)
??ldaformat2dtm
View(transcripts_lda)
summary(transcripts_lda)
length(transcripts_lda@terms)
transcripts_dtm <- mt_samples |>
select(transcription) |>
unnest_tokens(word, transcription) |>
filter(!(word %in% stopwords("english"))) |>
filter(!grepl("[[:digit:]]+", word)) |>
DocumentTermMatrix()
length(transcripts_lda@terms[0])
transcripts_lda@terms[0]
transcripts_lda@terms[3]
transcripts_lda@terms[9]
terms(transcripts_lda)
library(ggplot2)
transcripts_tops_terms <- as.data.frame(transcripts_lda@terms, transcripts_lda@beta) %>%
group_by(topic) %>%
slice(beta, n=10) %>%
ungroup() %>%
arrange(topic, beta)
transcripts_lda <- LDA(transcripts_dtm, k=9, control = list(seed = 1234))
terms(transcripts_lda)
length(terms(transcripts_lda))
length(beta(transcripts_lda))
transcripts_lda@beta
View(transcripts_lda)
library(broom)
library(ggplot2)
library(tidyr)
library(dplyr)
transcripts_tops_terms <-
tidy(transcripts_lda, matrix = "beta") |>
group_by(topic) |>
slice_max(beta, n = 10) |>
ungroup() |>
arrange(topic, -beta)
install.packages("reshape2")
transcripts_tops_terms <-
tidy(transcripts_lda, matrix = "beta") |>
group_by(topic) |>
slice_max(beta, n = 10) |>
ungroup() |>
arrange(topic, -beta)
