---
title: "Lab 08 - Text Mining/NLP"
output: html_document
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(eval = T, include  = T)
```

# Learning goals

- Use `unnest_tokens()` and `unnest_ngrams()` to extract tokens and ngrams from text
- Use dplyr and ggplot2 to analyze and visualize text data
- Try a theme model using `topicmodels`

# Lab description

For this lab we will be working with the medical record transcriptions from https://www.mtsamples.com/. And is loaded and "fairly" cleaned at https://github.com/JSC370/JSC370-2024/tree/main/data/medical_transcriptions.

This markdown document should be rendered using `github_document` document.



### Setup packages

You should load in `dplyr`, (or `data.table` if you want to work that way), `ggplot2` and `tidytext`. Install `wordcloud`, `tm`, and `topicmodels` if you don't alreadyh have them.



### Read in the Medical Transcriptions

Loading in reference transcription samples from https://www.mtsamples.com/

```{r}
library(tidytext)
library(tidyverse)
library(wordcloud)
library(tm)
library(topicmodels)

mt_samples <- read_csv("https://raw.githubusercontent.com/JSC370/JSC370-2024/main/data/medical_transcriptions/mtsamples.csv")
mt_samples <- mt_samples %>%
  select(description, medical_specialty, transcription)

head(mt_samples)
```

---

## Question 1: What specialties do we have?

We can use `count()` from `dplyr` to figure out how many different medical specialties are in the data. Are these categories related? overlapping? evenly distributed? Make a bar plot.

```{r}
mt_samples |>
  count(medical_specialty, sort = TRUE) %>% 
  head()
```


```{r}
mt_samples |>
  count(medical_specialty, sort = TRUE) |>
  ggplot(aes(x = reorder(medical_specialty, n), y = n)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Distribution of Medical Specialties",
       x = "Specialty",
       y = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Some of the categories are related like Neurosurgery and Surgery as well as Neurology.

Moreover, surgery has the highest distribution of medical specialties.


---

## Question 2: Tokenize

- Tokenize the the words in the `transcription` column
- Count the number of times each token appears
- Visualize the top 20 most frequent words with a bar plot
- Create a word cloud

Explain what we see from this result. Does it makes sense? What insights (if any) do we get?

```{r}
tokens <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |>
  count(word, sort = TRUE)

tokens |>
  slice_head(n = 20) %>% 
  ggplot(aes(x = reorder(word, n), y = n)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Top 20 Most Frequent Words",
       x = "Word",
       y = "Frequency") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability


wordcloud(words = tokens$word, freq = tokens$n, max.words = 100, colors = brewer.pal(8, "Dark2"))
```

"The", "and", "of" and "a" indeed are the most frequent words in English as they support English syntax with no domain specific meanings, so while the result may be correct, it is less informative for our specific objective. 

---

## Question 3: Stopwords

- Redo Question 2 but remove stopwords(use stopwords package)
- Bonus points if you remove numbers as well (use regex)

What do we see know that we have removed stop words? Does it give us a better idea of what the text is about?

- Try customizing your stopwords list to include 3-4 additional words that do not appear informative

```{r}
head(stopwords("english"))
length(stopwords("english"))
stopwords1 <- c(stopwords("english"), "left", "right", "also", "using")


tokens <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |>
  group_by(word) %>% 
  summarize(word_frequency = n()) %>% 
  arrange(across(word_frequency, desc)) %>% 
  filter(!(word %in% stopwords1)) |>
  filter(!(grepl("[[:digit:]]+", word))) %>% 
  head(20)

tokens |>
  ggplot(aes(x = reorder(word, -word_frequency), y = word_frequency)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Top 20 Most Frequent Words",
       x = "Word",
       y = "Frequency") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability


wordcloud(words = tokens$word, freq = tokens$word_frequency, max.words = 100, colors = brewer.pal(8, "Dark2"))
```

After removing stop words, we can see the most frequent words like "patient", "procedure", "pain", and "blood" can be explained as they are medical-related.

---



# Question 4: ngrams

Repeat question 2, but this time tokenize into bi-grams. How does the result change if you look at tri-grams? Note we need to remove stopwords a little differently. You don't need to recreate the wordclouds.

```{r}
stopwords2 <- c(stopwords("english"), "also", "using", "use", "used")

sw_start <- paste0("^", paste(stopwords2, collapse=" |^"), "$")
sw_end <- paste0("", paste(stopwords2, collapse="$| "), "$")

tokens_bigram <- mt_samples |>
  select(transcription) |>
  unnest_tokens(ngram, transcription, token = "ngrams", n = 2) |>
  filter(!(grepl(sw_start, ngram, ignore.case = TRUE)))|>
  filter(!(grepl(sw_end, ngram, ignore.case = TRUE)))|>
  filter(!(grepl("[[:digit:]]+", ngram)))|>
  group_by(ngram) %>% 
  summarize(word_frequency = n()) %>% 
  arrange(across(word_frequency, desc))

tokens_bigram |>
  head(20) %>% 
  ggplot(aes(x = reorder(ngram, -word_frequency), y = word_frequency)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Top 20 Most Frequent Words",
       x = "Word",
       y = "Frequency") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Tri-grams capture more context than bi-grams because they consider sequences of three words instead of just two. Tri-grams can provide more detailed information about the relationships between words in a text, but they also increase the dimensionality of the data, potentially leading to sparser representations, especially for smaller datasets. 

---

# Question 5: Examining words

Using the results from the bigram, pick a word and count the words that appear before and after it, and create a plot of the top 20.

```{r}
library(stringr)
# e.g. patient, blood, preoperative...
# word = "blood"

tokens_blood <- tokens_bigram |>
  filter(str_detect(ngram, regex("\\sblood$|^blood\\s"))) |>
    mutate(word = str_remove(ngram, "blood"),
         word = str_remove_all(word, " ")) |>
  arrange(across(word_frequency, desc))

tokens_blood %>% 
  head(20) %>% 
  ggplot(aes(x = reorder(word, -word_frequency), y = word_frequency)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Top 20 Most Frequent Words",
       x = "Word",
       y = "Frequency") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

---


# Question 6: Words by Specialties

Which words are most used in each of the specialties. you can use `group_by()` and `top_n()` from `dplyr` to have the calculations be done within each specialty. Remember to remove stopwords. How about the most 5 used words?


```{r}
mt_samples |>
  unnest_tokens(word, transcription) |>
  filter(!(word %in% stopwords("english"))) |>
  filter(!(grepl("[[:digit:]]+", word))) %>% 
  group_by(medical_specialty) |>
  count(word, sort = TRUE) %>% 
  top_n(1, n)
```
```{r}
mt_samples |>
  unnest_tokens(word, transcription) |>
  filter(!(word %in% stopwords("english"))) |>
  filter(!(grepl("[[:digit:]]+", word))) %>% 
  group_by(medical_specialty) |>
  count(word, sort = TRUE) %>% 
  top_n(5, n)
```

# Question 7: Topic Models

See if there are any themes in the data by using a topic model (LDA). 

- you first need to create a document term matrix
- then you can try the LDA function in `topicmodels`. Try different k values.
- create a facet plot of the results from the LDA (see code from lecture)


```{r}
transcripts_dtm <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |>
  filter(!(word %in% stopwords("english"))) |>
  filter(!grepl("[[:digit:]]+", word)) |>
  DocumentTermMatrix()


transcripts_dtm <- as.matrix(transcripts_dtm)   

transcripts_lda <- LDA(transcripts_dtm, k=6, control = list(seed = 1234))
transcripts_lda
```
```{r}
transcripts_tops_terms <- 
  tidy(transcripts_lda, matrix = "beta") |>
  group_by(topic) |>
  slice_max(beta, n = 10) |> 
  ungroup() |>
  arrange(topic, -beta)
transcripts_tops_terms |>
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```



# Deliverables

1. Questions 1-7 answered, raw .Rmd file and pdf or html output uploaded to Quercus
